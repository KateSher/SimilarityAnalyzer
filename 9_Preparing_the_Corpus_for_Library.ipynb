{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"9_Preparing_the_Corpus_for_Library.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMQov/Zt2aTXf6XjCnnMH+O"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"3OUUJlHnLVbT"},"source":["## Loading the required libraries"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"mg0IobdDnJLy","executionInfo":{"status":"ok","timestamp":1658669299278,"user_tz":-180,"elapsed":538,"user":{"displayName":"Kateryna Sheremet","userId":"03402668552086225828"}}},"outputs":[],"source":["import linecache\n","from gensim.corpora.wikicorpus import WikiCorpus, tokenize"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23891,"status":"ok","timestamp":1658157303983,"user":{"displayName":"Kateryna Sheremet","userId":"03402668552086225828"},"user_tz":-180},"id":"RxM-W3nMtZaI","outputId":"b64b1e95-f43e-4f22-d0c7-a465f514cead"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["# Google drive mount\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","source":["## Preparing the corpus for use in the library\n","\n","Converting Wikipedia article dump from Wikimedia XML format into a text file with obtaining IDs, titles and content. \n","The received text file will be used in the **find_text** method of the **text_relevance** library."],"metadata":{"id":"IKp-2LNvXd91"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pno0USE8DtvT"},"outputs":[],"source":["wiki = WikiCorpus('drive/MyDrive/Colab Notebooks/enwiki-20220320-pages-articles-multistream9.xml-p2936261p4045402.bz2',  # path to the file you downloaded above\n","                  tokenizer_func=tokenize,  # simple regexp; plug in your own tokenizer here\n","                  dictionary={},  # don't start processing the data yet\n","                  )\n","wiki.metadata = True # also return the article titles and ids when parsing\n","\n","with open('drive/MyDrive/Colab Notebooks/wiki(num_title).txt', 'w', encoding='utf8') as fout:  \n","    for article_no, (content, (page_id_and_title)) in enumerate(wiki.get_texts()):\n","        title = page_id_and_title[1]\n","        content = ' '.join(content)\n","        fout.write(f'{article_no} || {title} || {content}\\n')  # article no. || title of article || words of the article"]}]}